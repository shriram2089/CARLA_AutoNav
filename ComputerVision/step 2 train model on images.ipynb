{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 90, 160, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 90, 160, 64)  1792        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 45, 80, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 22, 40, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 11, 20, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 14080)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14081)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           901248      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 976,961\n",
      "Trainable params: 976,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "459/459 [==============================] - 13s 11ms/step - loss: 0.0331 - MSE: 0.0331 - val_loss: 0.0138 - val_MSE: 0.0138\n",
      "Epoch 2/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0090 - MSE: 0.0090 - val_loss: 0.0114 - val_MSE: 0.0114\n",
      "Epoch 3/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0071 - MSE: 0.0071 - val_loss: 0.0112 - val_MSE: 0.0112\n",
      "Epoch 4/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0064 - MSE: 0.0064 - val_loss: 0.0117 - val_MSE: 0.0117\n",
      "Epoch 5/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0057 - MSE: 0.0057 - val_loss: 0.0107 - val_MSE: 0.0107\n",
      "Epoch 6/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0048 - MSE: 0.0048 - val_loss: 0.0093 - val_MSE: 0.0093\n",
      "Epoch 7/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0045 - MSE: 0.0045 - val_loss: 0.0088 - val_MSE: 0.0088\n",
      "Epoch 8/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0044 - MSE: 0.0044 - val_loss: 0.0088 - val_MSE: 0.0088\n",
      "Epoch 9/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0038 - MSE: 0.0038 - val_loss: 0.0103 - val_MSE: 0.0103\n",
      "Epoch 10/10\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 0.0034 - MSE: 0.0034 - val_loss: 0.0101 - val_MSE: 0.0101\n",
      "144/144 [==============================] - 1s 8ms/step\n",
      "Prediction min:  -0.5970035  Prediction max:  0.79195476\n"
     ]
    }
   ],
   "source": [
    "# First attempt to train a model based on images generated\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 90\n",
    "WIDTH = 160\n",
    "\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "# get a list when both are available: image and steering\n",
    "\n",
    "# read training data \n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    # this version adds taking lower side of the image\n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    Y.append(y)\n",
    "\n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=30, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsXklEQVR4nO3deXRUdZ7//1cSkkpYKhFCUmQITBSbRUEkNqEcFZcMgQ49doNnRBGjog5MsBtiA2aaRoWeA+KCG4qjSOhfyyDYroRFBMGFAjUSRbYjGjvYWMFAk2JNCHx+f/Q3dygJkAqV5ROej3PuOdS97/rk886lUq9z695bEcYYIwAAAItENvUEAAAAQkWAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYp1VTT6ChnDhxQrt371a7du0UERHR1NMBAAB1YIzRgQMHlJKSosjI0x9nabEBZvfu3UpNTW3qaQAAgHrYtWuXOnfufNrtLTbAtGvXTtI/fgFut7uJZwMAAOoiEAgoNTXVeR8/nRYbYGo+NnK73QQYAAAsc7bTPziJFwAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1Wuy3UQNo2UpLS1VeXh72cRMTE9WlS5ewjwsgvAgwAKxTWlqq7j166uiRw2EfOzautXZs30aIAZo5AgwA65SXl+vokcPqMPR+RXdIDdu4x/bu0t6lj6u8vJwAAzRzBBgA1orukCqXp1tTTwNAE+AkXgAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYJ6QA89BDDykiIiJo6dGjh7P96NGjys3NVYcOHdS2bVsNHz5cZWVlQWOUlpYqOztbrVu3VlJSkiZOnKjq6uqgmrVr16pfv35yuVzq1q2bCgoK6t8hAABocUI+AnPJJZfohx9+cJaPPvrI2TZhwgS98847WrJkidatW6fdu3dr2LBhzvbjx48rOztbVVVVWr9+vRYsWKCCggJNnTrVqSkpKVF2drauu+46FRcXa/z48br77ru1cuXKc2wVAAC0FCF/G3WrVq3k8XhOWV9RUaF58+Zp4cKFuv766yVJ8+fPV8+ePbVhwwYNGDBA7777rrZu3ar33ntPycnJ6tu3r6ZPn67JkyfroYceUkxMjObOnau0tDQ9/vjjkqSePXvqo48+0uzZs5WVlXWO7QIAgJYg5CMwX3/9tVJSUnThhRdq5MiRKi0tlSQVFRXp2LFjyszMdGp79OihLl26yOfzSZJ8Pp969+6t5ORkpyYrK0uBQEBbtmxxak4eo6amZozTqaysVCAQCFoAAEDLFFKAycjIUEFBgVasWKHnn39eJSUluvrqq3XgwAH5/X7FxMQoISEh6DnJycny+/2SJL/fHxRearbXbDtTTSAQ0JEjR047txkzZig+Pt5ZUlNTQ2kNAABYJKSPkIYMGeL8u0+fPsrIyFDXrl21ePFixcXFhX1yocjPz1deXp7zOBAIEGIAAGihzuky6oSEBP3sZz/Tzp075fF4VFVVpf379wfVlJWVOefMeDyeU65Kqnl8thq3233GkORyueR2u4MWAADQMp1TgDl48KC++eYbderUSenp6YqOjtbq1aud7Tt27FBpaam8Xq8kyev1avPmzdqzZ49Ts2rVKrndbvXq1cupOXmMmpqaMQAAAEIKML/73e+0bt06fffdd1q/fr1+/etfKyoqSrfccovi4+M1evRo5eXl6f3331dRUZHuvPNOeb1eDRgwQJI0aNAg9erVS6NGjdIXX3yhlStXasqUKcrNzZXL5ZIkjRkzRt9++60mTZqk7du367nnntPixYs1YcKE8HcPAACsFNI5MN9//71uueUW7d27Vx07dtRVV12lDRs2qGPHjpKk2bNnKzIyUsOHD1dlZaWysrL03HPPOc+PiorS0qVLNXbsWHm9XrVp00Y5OTmaNm2aU5OWlqbCwkJNmDBBTz31lDp37qyXXnqJS6gBAIAjwhhjmnoSDSEQCCg+Pl4VFRWcDwO0MJ9//rnS09PlyXlSLk+3sI1b6d8p/4LxKioqUr9+/cI2LoC6q+v7N9+FBAAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABY55wCzMyZMxUREaHx48c7644eParc3Fx16NBBbdu21fDhw1VWVhb0vNLSUmVnZ6t169ZKSkrSxIkTVV1dHVSzdu1a9evXTy6XS926dVNBQcG5TBUAALQg9Q4wn376qV544QX16dMnaP2ECRP0zjvvaMmSJVq3bp12796tYcOGOduPHz+u7OxsVVVVaf369VqwYIEKCgo0depUp6akpETZ2dm67rrrVFxcrPHjx+vuu+/WypUr6ztdAADQgtQrwBw8eFAjR47Uiy++qAsuuMBZX1FRoXnz5umJJ57Q9ddfr/T0dM2fP1/r16/Xhg0bJEnvvvuutm7dqj//+c/q27evhgwZounTp2vOnDmqqqqSJM2dO1dpaWl6/PHH1bNnT40bN0433XSTZs+eHYaWAQCA7eoVYHJzc5Wdna3MzMyg9UVFRTp27FjQ+h49eqhLly7y+XySJJ/Pp969eys5OdmpycrKUiAQ0JYtW5yan46dlZXljFGbyspKBQKBoAUAALRMrUJ9wqJFi/T555/r008/PWWb3+9XTEyMEhISgtYnJyfL7/c7NSeHl5rtNdvOVBMIBHTkyBHFxcWd8rNnzJihhx9+ONR2AACAhUI6ArNr1y799re/1SuvvKLY2NiGmlO95Ofnq6Kiwll27drV1FMCAAANJKQAU1RUpD179qhfv35q1aqVWrVqpXXr1unpp59Wq1atlJycrKqqKu3fvz/oeWVlZfJ4PJIkj8dzylVJNY/PVuN2u2s9+iJJLpdLbrc7aAEAAC1TSAHmhhtu0ObNm1VcXOwsV1xxhUaOHOn8Ozo6WqtXr3aes2PHDpWWlsrr9UqSvF6vNm/erD179jg1q1atktvtVq9evZyak8eoqakZAwAAnN9COgemXbt2uvTSS4PWtWnTRh06dHDWjx49Wnl5eWrfvr3cbrfuu+8+eb1eDRgwQJI0aNAg9erVS6NGjdKsWbPk9/s1ZcoU5ebmyuVySZLGjBmjZ599VpMmTdJdd92lNWvWaPHixSosLAxHzwAAwHIhn8R7NrNnz1ZkZKSGDx+uyspKZWVl6bnnnnO2R0VFaenSpRo7dqy8Xq/atGmjnJwcTZs2zalJS0tTYWGhJkyYoKeeekqdO3fWSy+9pKysrHBPFwAAWOicA8zatWuDHsfGxmrOnDmaM2fOaZ/TtWtXLVu27IzjXnvttdq0adO5Tg8AALRAfBcSAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGCdkALM888/rz59+sjtdsvtdsvr9Wr58uXO9qNHjyo3N1cdOnRQ27ZtNXz4cJWVlQWNUVpaquzsbLVu3VpJSUmaOHGiqqurg2rWrl2rfv36yeVyqVu3biooKKh/hwAAoMUJKcB07txZM2fOVFFRkT777DNdf/31uvHGG7VlyxZJ0oQJE/TOO+9oyZIlWrdunXbv3q1hw4Y5zz9+/Liys7NVVVWl9evXa8GCBSooKNDUqVOdmpKSEmVnZ+u6665TcXGxxo8fr7vvvlsrV64MU8sAAMB2EcYYcy4DtG/fXo8++qhuuukmdezYUQsXLtRNN90kSdq+fbt69uwpn8+nAQMGaPny5Ro6dKh2796t5ORkSdLcuXM1efJk/fjjj4qJidHkyZNVWFior776yvkZI0aM0P79+7VixYo6zysQCCg+Pl4VFRVyu93n0iKAZubzzz9Xenq6PDlPyuXpFrZxK/075V8wXkVFRerXr1/YxgVQd3V9/673OTDHjx/XokWLdOjQIXm9XhUVFenYsWPKzMx0anr06KEuXbrI5/NJknw+n3r37u2EF0nKyspSIBBwjuL4fL6gMWpqasY4ncrKSgUCgaAFAAC0TCEHmM2bN6tt27ZyuVwaM2aM3njjDfXq1Ut+v18xMTFKSEgIqk9OTpbf75ck+f3+oPBSs71m25lqAoGAjhw5ctp5zZgxQ/Hx8c6SmpoaamsAAMASIQeY7t27q7i4WBs3btTYsWOVk5OjrVu3NsTcQpKfn6+Kigpn2bVrV1NPCQAANJBWoT4hJiZG3br94zPn9PR0ffrpp3rqqad08803q6qqSvv37w86ClNWViaPxyNJ8ng8+uSTT4LGq7lK6eSan165VFZWJrfbrbi4uNPOy+VyyeVyhdoOAACw0DnfB+bEiROqrKxUenq6oqOjtXr1amfbjh07VFpaKq/XK0nyer3avHmz9uzZ49SsWrVKbrdbvXr1cmpOHqOmpmYMAACAkI7A5Ofna8iQIerSpYsOHDighQsXau3atVq5cqXi4+M1evRo5eXlqX379nK73brvvvvk9Xo1YMAASdKgQYPUq1cvjRo1SrNmzZLf79eUKVOUm5vrHD0ZM2aMnn32WU2aNEl33XWX1qxZo8WLF6uwsDD83QMAACuFFGD27Nmj22+/XT/88IPi4+PVp08frVy5Uv/6r/8qSZo9e7YiIyM1fPhwVVZWKisrS88995zz/KioKC1dulRjx46V1+tVmzZtlJOTo2nTpjk1aWlpKiws1IQJE/TUU0+pc+fOeumll5SVlRWmlgEAgO3O+T4wzRX3gQFaLu4DA7RcDX4fGAAAgKZCgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWCekADNjxgz9/Oc/V7t27ZSUlKRf/epX2rFjR1DN0aNHlZubqw4dOqht27YaPny4ysrKgmpKS0uVnZ2t1q1bKykpSRMnTlR1dXVQzdq1a9WvXz+5XC5169ZNBQUF9esQAAC0OCEFmHXr1ik3N1cbNmzQqlWrdOzYMQ0aNEiHDh1yaiZMmKB33nlHS5Ys0bp167R7924NGzbM2X78+HFlZ2erqqpK69ev14IFC1RQUKCpU6c6NSUlJcrOztZ1112n4uJijR8/XnfffbdWrlwZhpYBAIDtIowxpr5P/vHHH5WUlKR169bpmmuuUUVFhTp27KiFCxfqpptukiRt375dPXv2lM/n04ABA7R8+XINHTpUu3fvVnJysiRp7ty5mjx5sn788UfFxMRo8uTJKiws1FdffeX8rBEjRmj//v1asWJFneYWCAQUHx+viooKud3u+rYIoBn6/PPPlZ6eLk/Ok3J5uoVt3Er/TvkXjFdRUZH69esXtnEB1F1d37/P6RyYiooKSVL79u0lSUVFRTp27JgyMzOdmh49eqhLly7y+XySJJ/Pp969ezvhRZKysrIUCAS0ZcsWp+bkMWpqasaoTWVlpQKBQNACAABapnoHmBMnTmj8+PH6l3/5F1166aWSJL/fr5iYGCUkJATVJicny+/3OzUnh5ea7TXbzlQTCAR05MiRWuczY8YMxcfHO0tqamp9WwMAAM1cvQNMbm6uvvrqKy1atCic86m3/Px8VVRUOMuuXbuaekoAAKCBtKrPk8aNG6elS5fqgw8+UOfOnZ31Ho9HVVVV2r9/f9BRmLKyMnk8Hqfmk08+CRqv5iqlk2t+euVSWVmZ3G634uLiap2Ty+WSy+WqTzsAAMAyIR2BMcZo3LhxeuONN7RmzRqlpaUFbU9PT1d0dLRWr17trNuxY4dKS0vl9XolSV6vV5s3b9aePXucmlWrVsntdqtXr15Ozclj1NTUjAEAAM5vIR2Byc3N1cKFC/XWW2+pXbt2zjkr8fHxiouLU3x8vEaPHq28vDy1b99ebrdb9913n7xerwYMGCBJGjRokHr16qVRo0Zp1qxZ8vv9mjJlinJzc50jKGPGjNGzzz6rSZMm6a677tKaNWu0ePFiFRYWhrl9AABgo5COwDz//POqqKjQtddeq06dOjnLq6++6tTMnj1bQ4cO1fDhw3XNNdfI4/Ho9ddfd7ZHRUVp6dKlioqKktfr1W233abbb79d06ZNc2rS0tJUWFioVatW6bLLLtPjjz+ul156SVlZWWFoGQAA2C6kIzB1uWVMbGys5syZozlz5py2pmvXrlq2bNkZx7n22mu1adOmUKYHAADOE3wXEgAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWKdVU08AQNMrLS1VeXl52MdNTExUly5dwj4uABBggPNcaWmpuvfoqaNHDod97Ni41tqxfRshBkDYEWCA81x5ebmOHjmsDkPvV3SH1LCNe2zvLu1d+rjKy8sJMADCjgADQJIU3SFVLk+3pp4GANQJJ/ECAADrhBxgPvjgA/3yl79USkqKIiIi9OabbwZtN8Zo6tSp6tSpk+Li4pSZmamvv/46qGbfvn0aOXKk3G63EhISNHr0aB08eDCo5ssvv9TVV1+t2NhYpaamatasWaF3BwAAWqSQA8yhQ4d02WWXac6cObVunzVrlp5++mnNnTtXGzduVJs2bZSVlaWjR486NSNHjtSWLVu0atUqLV26VB988IHuvfdeZ3sgENCgQYPUtWtXFRUV6dFHH9VDDz2k//mf/6lHiwAAoKUJ+RyYIUOGaMiQIbVuM8boySef1JQpU3TjjTdKkv70pz8pOTlZb775pkaMGKFt27ZpxYoV+vTTT3XFFVdIkp555hn94he/0GOPPaaUlBS98sorqqqq0ssvv6yYmBhdcsklKi4u1hNPPBEUdAAAwPkprCfxlpSUyO/3KzMz01kXHx+vjIwM+Xw+jRgxQj6fTwkJCU54kaTMzExFRkZq48aN+vWvfy2fz6drrrlGMTExTk1WVpYeeeQR/f3vf9cFF1xwys+urKxUZWWl8zgQCISzNQDnkW3btjXIuNwXBwifsAYYv98vSUpOTg5an5yc7Gzz+/1KSkoKnkSrVmrfvn1QTVpa2ilj1GyrLcDMmDFDDz/8cHgaARA2DREGGipgHD/4dykiQrfddluDjM99cYDwaTGXUefn5ysvL895HAgElJoavntaAAhNQ4eBhnCi8qBkTNjviSNxXxwg3MIaYDwejySprKxMnTp1ctaXlZWpb9++Ts2ePXuCnlddXa19+/Y5z/d4PCorKwuqqXlcU/NTLpdLLpcrLH0AOHcNGQaOfPuZKj78c1jHPBn3xAGav7AGmLS0NHk8Hq1evdoJLIFAQBs3btTYsWMlSV6vV/v371dRUZHS09MlSWvWrNGJEyeUkZHh1Pz+97/XsWPHFB0dLUlatWqVunfvXuvHRwCar4YIA8f27grreADsE/Jl1AcPHlRxcbGKi4sl/ePE3eLiYpWWlioiIkLjx4/XH//4R7399tvavHmzbr/9dqWkpOhXv/qVJKlnz54aPHiw7rnnHn3yySf6+OOPNW7cOI0YMUIpKSmSpFtvvVUxMTEaPXq0tmzZoldffVVPPfVU0EdEAADg/BXyEZjPPvtM1113nfO4JlTk5OSooKBAkyZN0qFDh3Tvvfdq//79uuqqq7RixQrFxsY6z3nllVc0btw43XDDDYqMjNTw4cP19NNPO9vj4+P17rvvKjc3V+np6UpMTNTUqVO5hBoAAEiqR4C59tprZYw57faIiAhNmzZN06ZNO21N+/bttXDhwjP+nD59+ujDDz8MdXoAAOA8wHchAQAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1WjX1BAAA56fS0lKVl5eHfdzExER16dIl7OOieSHAAAAaXWlpqbr36KmjRw6HfezYuNbasX0bIaaFI8AAABpdeXm5jh45rA5D71d0h9SwjXts7y7tXfq4ysvLCTAtHAEGsERDHW7ftm1b2McE6iq6Q6pcnm5NPQ1YiAADWKAhD7cDgI0IMDgvNdTRDKlhTiBsqMPtknTk289U8eGfwzomADQ0AgzOOw19NMPlitVf/vKaOnXqFLYxaz7maYjD7cf27grreADQGAgwOO805NGMo99v0f41L2no0KFhHRcAEIwAg/NWgx3NMCbs4YiPeVqOhjhpurKyUi6XK+zjStxTBc0XAQZoAOEOR3zMY7/jB/8uRUTotttuC//gEZGSORH+ccU9VdB8EWAAoBGcqDzYoEfnGuIjUe6pguaMAAMAjaihjs5xPxWcbwgwAIAzaojzdriBIs4VAQYAUKsGPW8HOEcEGABArRrqvB2JK+tw7ggwAIAz4gaKaI4IMACAFqehzrHhvjjNBwEGANBiNPR5O9wXp/kgwAAAWoyGPG+H++I0LwQYAECLw31xWr7Ipp4AAABAqDgCg2attLRU5eXlYR2TG2gBgP2adYCZM2eOHn30Ufn9fl122WV65pln1L9//6aeFhpJaWmpuvfoqaNHDjf1VAAAzUyzDTCvvvqq8vLyNHfuXGVkZOjJJ59UVlaWduzYoaSkpKaeHhpBeXm5jh453GBffgcA9dEQR3G5PDt0zTbAPPHEE7rnnnt05513SpLmzp2rwsJCvfzyy3rggQeaeHY4WUN8zCP93x+JhvryOwAIRUNeou1yxeovf3lNnTp1CvvYLTUcNcsAU1VVpaKiIuXn5zvrIiMjlZmZKZ/PV+tzKisrVVlZ6TyuqKiQJAUCgbDPz+/3y+/3h31c6R99njhxwppxy8rKdNuo21VVeTTsY9eo9O/UiarwjV8TYMI9bkOOzZwbZ2zmbP/YDTnnyt3bJGPk/vkwRcV3DNu4x378Tge/WKmhQ4eGbcyTxbhi9ef/709KTk4O67gej0cejyesY0r/975tjDlzoWmG/va3vxlJZv369UHrJ06caPr371/rcx588EEjiYWFhYWFhaUFLLt27TpjVmiWR2DqIz8/X3l5ec7jEydOaN++ferQoYMiIiLC9nMCgYBSU1O1a9cuud3usI3bnLT0HunPfi29x5ben9Tye6S/+jPG6MCBA0pJSTljXbMMMImJiYqKilJZWVnQ+rKystMernK5XHK5XEHrEhISGmqKcrvdLfI/5claeo/0Z7+W3mNL709q+T3SX/3Ex8eftaZZ3sguJiZG6enpWr16tbPuxIkTWr16tbxebxPODAAANAfN8giMJOXl5SknJ0dXXHGF+vfvryeffFKHDh1yrkoCAADnr2YbYG6++Wb9+OOPmjp1qvx+v/r27asVK1aE/SzqULlcLj344IOnfFzVkrT0HunPfi29x5ben9Tye6S/hhdhzNmuUwIAAGhemuU5MAAAAGdCgAEAANYhwAAAAOsQYAAAgHUIMLX47//+b1155ZVq3bp1nW+GZ4zR1KlT1alTJ8XFxSkzM1Nff/11UM2+ffs0cuRIud1uJSQkaPTo0Tp48GADdHBmoc7ju+++U0RERK3LkiVLnLrati9atKgxWgpSn9/ztddee8rcx4wZE1RTWlqq7OxstW7dWklJSZo4caKqq6sbspXTCrXHffv26b777lP37t0VFxenLl266De/+Y3znWE1mmofzpkzR//8z/+s2NhYZWRk6JNPPjlj/ZIlS9SjRw/Fxsaqd+/eWrZsWdD2urweG1soPb744ou6+uqrdcEFF+iCCy5QZmbmKfV33HHHKftq8ODBDd3GaYXSX0FBwSlzj42NDaqxfR/W9jclIiJC2dnZTk1z2YcffPCBfvnLXyolJUURERF68803z/qctWvXql+/fnK5XOrWrZsKCgpOqQn1dR2yMHx1UYszdepU88QTT5i8vDwTHx9fp+fMnDnTxMfHmzfffNN88cUX5t/+7d9MWlqaOXLkiFMzePBgc9lll5kNGzaYDz/80HTr1s3ccsstDdTF6YU6j+rqavPDDz8ELQ8//LBp27atOXDggFMnycyfPz+o7uT+G0t9fs8DBw4099xzT9DcKyoqnO3V1dXm0ksvNZmZmWbTpk1m2bJlJjEx0eTn5zd0O7UKtcfNmzebYcOGmbffftvs3LnTrF692lx88cVm+PDhQXVNsQ8XLVpkYmJizMsvv2y2bNli7rnnHpOQkGDKyspqrf/4449NVFSUmTVrltm6dauZMmWKiY6ONps3b3Zq6vJ6bEyh9njrrbeaOXPmmE2bNplt27aZO+64w8THx5vvv//eqcnJyTGDBw8O2lf79u1rrJaChNrf/PnzjdvtDpq73+8PqrF9H+7duzeov6+++spERUWZ+fPnOzXNZR8uW7bM/P73vzevv/66kWTeeOONM9Z/++23pnXr1iYvL89s3brVPPPMMyYqKsqsWLHCqQn191UfBJgzmD9/fp0CzIkTJ4zH4zGPPvqos27//v3G5XKZ//3f/zXGGLN161YjyXz66adOzfLly01ERIT529/+Fva5n0645tG3b19z1113Ba2ry3/8hlbf/gYOHGh++9vfnnb7smXLTGRkZNAf2eeff9643W5TWVkZlrnXVbj24eLFi01MTIw5duyYs64p9mH//v1Nbm6u8/j48eMmJSXFzJgxo9b6f//3fzfZ2dlB6zIyMsx//Md/GGPq9npsbKH2+FPV1dWmXbt2ZsGCBc66nJwcc+ONN4Z7qvUSan9n+9vaEvfh7NmzTbt27czBgweddc1pH9aoy9+ASZMmmUsuuSRo3c0332yysrKcx+f6+6oLPkIKg5KSEvn9fmVmZjrr4uPjlZGRIZ/PJ0ny+XxKSEjQFVdc4dRkZmYqMjJSGzdubLS5hmMeRUVFKi4u1ujRo0/Zlpubq8TERPXv318vv/zy2b8OPczOpb9XXnlFiYmJuvTSS5Wfn6/Dhw8Hjdu7d++gGylmZWUpEAhoy5Yt4W/kDML1f6miokJut1utWgXfz7Ix92FVVZWKioqCXjuRkZHKzMx0Xjs/5fP5guqlf+yLmvq6vB4bU316/KnDhw/r2LFjat++fdD6tWvXKikpSd27d9fYsWO1d+/esM69Lurb38GDB9W1a1elpqbqxhtvDHodtcR9OG/ePI0YMUJt2rQJWt8c9mGozvYaDMfvqy6a7Z14beL3+yXplLsEJycnO9v8fr+SkpKCtrdq1Urt27d3ahpDOOYxb9489ezZU1deeWXQ+mnTpun6669X69at9e677+o///M/dfDgQf3mN78J2/zPpr793XrrreratatSUlL05ZdfavLkydqxY4def/11Z9za9m/NtsYUjn1YXl6u6dOn69577w1a39j7sLy8XMePH6/1d7t9+/Zan3O6fXHya61m3elqGlN9evypyZMnKyUlJegNYfDgwRo2bJjS0tL0zTff6L/+6780ZMgQ+Xw+RUVFhbWHM6lPf927d9fLL7+sPn36qKKiQo899piuvPJKbdmyRZ07d25x+/CTTz7RV199pXnz5gWtby77MFSnew0GAgEdOXJEf//738/5/3xdnDcB5oEHHtAjjzxyxppt27apR48ejTSj8Kprf+fqyJEjWrhwof7whz+csu3kdZdffrkOHTqkRx99NCxvfg3d38lv5L1791anTp10ww036JtvvtFFF11U73FD0Vj7MBAIKDs7W7169dJDDz0UtK0h9yHqZ+bMmVq0aJHWrl0bdKLriBEjnH/37t1bffr00UUXXaS1a9fqhhtuaIqp1pnX6w36Yt4rr7xSPXv21AsvvKDp06c34cwaxrx589S7d2/1798/aL3N+7A5OG8CzP3336877rjjjDUXXnhhvcb2eDySpLKyMnXq1MlZX1ZWpr59+zo1e/bsCXpedXW19u3b5zz/XNS1v3Odx2uvvabDhw/r9ttvP2ttRkaGpk+frsrKynP+vozG6q9GRkaGJGnnzp266KKL5PF4TjmDvqysTJLCsv+kxunxwIEDGjx4sNq1a6c33nhD0dHRZ6wP5z6sTWJioqKiopzfZY2ysrLT9uLxeM5YX5fXY2OqT481HnvsMc2cOVPvvfee+vTpc8baCy+8UImJidq5c2ejvvmdS381oqOjdfnll2vnzp2SWtY+PHTokBYtWqRp06ad9ec01T4M1eleg263W3FxcYqKijrn/xN1ErazaVqgUE/ifeyxx5x1FRUVtZ7E+9lnnzk1K1eubLKTeOs7j4EDB55y5crp/PGPfzQXXHBBvedaH+H6PX/00UdGkvniiy+MMf93Eu/JZ9C/8MILxu12m6NHj4avgTqob48VFRVmwIABZuDAgebQoUN1+lmNsQ/79+9vxo0b5zw+fvy4+ad/+qcznsQ7dOjQoHVer/eUk3jP9HpsbKH2aIwxjzzyiHG73cbn89XpZ+zatctERESYt95665znG6r69Hey6upq0717dzNhwgRjTMvZh8b8433E5XKZ8vLys/6MptyHNVTHk3gvvfTSoHW33HLLKSfxnsv/iTrNNWwjtSB//etfzaZNm5xLhTdt2mQ2bdoUdMlw9+7dzeuvv+48njlzpklISDBvvfWW+fLLL82NN95Y62XUl19+udm4caP56KOPzMUXX9xkl1GfaR7ff/+96d69u9m4cWPQ877++msTERFhli9ffsqYb7/9tnnxxRfN5s2bzddff22ee+4507p1azN16tQG7+enQu1v586dZtq0aeazzz4zJSUl5q233jIXXnihueaaa5zn1FxGPWjQIFNcXGxWrFhhOnbs2KSXUYfSY0VFhcnIyDC9e/c2O3fuDLpss7q62hjTdPtw0aJFxuVymYKCArN161Zz7733moSEBOeKr1GjRpkHHnjAqf/4449Nq1atzGOPPWa2bdtmHnzwwVovoz7b67ExhdrjzJkzTUxMjHnttdeC9lXN36ADBw6Y3/3ud8bn85mSkhLz3nvvmX79+pmLL7640QN1ffp7+OGHzcqVK80333xjioqKzIgRI0xsbKzZsmWLU2P7Pqxx1VVXmZtvvvmU9c1pHx44cMB5n5NknnjiCbNp0ybz17/+1RhjzAMPPGBGjRrl1NdcRj1x4kSzbds2M2fOnFovoz7T7yscCDC1yMnJMZJOWd5//32nRv/vfhk1Tpw4Yf7whz+Y5ORk43K5zA033GB27NgRNO7evXvNLbfcYtq2bWvcbre58847g0JRYznbPEpKSk7p1xhj8vPzTWpqqjl+/PgpYy5fvtz07dvXtG3b1rRp08ZcdtllZu7cubXWNrRQ+ystLTXXXHONad++vXG5XKZbt25m4sSJQfeBMcaY7777zgwZMsTExcWZxMREc//99wddgtyYQu3x/fffr/X/tCRTUlJijGnaffjMM8+YLl26mJiYGNO/f3+zYcMGZ9vAgQNNTk5OUP3ixYvNz372MxMTE2MuueQSU1hYGLS9Lq/HxhZKj127dq11Xz344IPGGGMOHz5sBg0aZDp27Giio6NN165dzT333BPWN4dQhdLf+PHjndrk5GTzi1/8wnz++edB49m+D40xZvv27UaSeffdd08Zqzntw9P9fajpJycnxwwcOPCU5/Tt29fExMSYCy+8MOj9sMaZfl/hEGFMI1/nCgAAcI64DwwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1vn/AV3WZX+8ZziNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check distribution of our labels \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This shows a clear inbalance of labels - the model will learn to go straight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 90, 160, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 90, 160, 64)  1792        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 45, 80, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 22, 40, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 11, 20, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 14080)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14081)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           901248      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 976,961\n",
      "Trainable params: 976,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "536/536 [==============================] - 18s 17ms/step - loss: 14.7461 - MSE: 3145.3867 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 2/30\n",
      "536/536 [==============================] - 9s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 3/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 4/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 5/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 6/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 7/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 8/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 9/30\n",
      "536/536 [==============================] - 9s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 10/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 11/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 12/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 13/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 14/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 15/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 16/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 17/30\n",
      "536/536 [==============================] - 8s 15ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 18/30\n",
      "536/536 [==============================] - 8s 15ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 19/30\n",
      "536/536 [==============================] - 8s 15ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 20/30\n",
      "536/536 [==============================] - 8s 15ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 21/30\n",
      "536/536 [==============================] - 8s 15ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 22/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 23/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 24/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 25/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 26/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 27/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 28/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 29/30\n",
      "536/536 [==============================] - 8s 15ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "Epoch 30/30\n",
      "536/536 [==============================] - 8s 16ms/step - loss: 14.7726 - MSE: 3286.4277 - val_loss: 14.7333 - val_MSE: 3297.1626\n",
      "335/335 [==============================] - 3s 8ms/step\n",
      "Prediction min:  4.887908  Prediction max:  71.72839\n"
     ]
    }
   ],
   "source": [
    "# Second attempt to train a model based on images generated\n",
    "# here we will try to weigh down over-represented \"drive straight\" images\n",
    "# I used weighted_binary_crossentropy function BUT it HAS NOT worked\n",
    "# probably because it is for classification rather than regression\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 90\n",
    "WIDTH = 160\n",
    "\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "# get a list when both are available: image and steering\n",
    "\n",
    "# read training data \n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    # this version adds taking lower side of the image\n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    Y.append(y)\n",
    "\n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# this is a function I used before in semantic segmentation training\n",
    "def weighted_binary_crossentropy( y_true, y_pred, weight=1. ) :\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1-K.epsilon())\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "    logloss = -(y_true * K.log(y_pred) * weight + (1 - y_true) * K.log(1 - y_pred))\n",
    "    return K.mean( logloss, axis=-1)\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=weighted_binary_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=30, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how training set is distributed:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 180, 320, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 90, 160, 64)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 56320)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 56321)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           3604608     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,680,321\n",
      "Trainable params: 3,680,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# attempt 3\n",
    "# we will chop the training set limiting images with straight steering\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "EVERY_WHAT = 5 # we will keep every n'th image with zero angle \n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "straight_counter = 0 # we will be counting images to limit by applying \"every n'th\"\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    # a rough balancing by reducing number of zero steer examples\n",
    "    if (abs(y)>0.05 or straight_counter % EVERY_WHAT ==0) and abs(y)<0.5:\n",
    "        X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "        # gen direction values are taken from after 1st '_' in file name\n",
    "        X1.append(int(example.split('_')[1]))\n",
    "        Y.append(y)\n",
    "    if abs(y)>=0.05:\n",
    "        straight_counter +=1\n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "#model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=30, validation_split=0.2)\n",
    "\n",
    "#predictions = model.predict([X,X1])\n",
    "#print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_overfit\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_overfit\\assets\n"
     ]
    }
   ],
   "source": [
    "# to save model\n",
    "model.save(\"GPS_Visual_Model_overfit\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 109. GiB for an array with shape (84518, 180, 320, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7872\\3185005601.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#convert to numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0mX1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 109. GiB for an array with shape (84518, 180, 320, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "# attempt 4\n",
    "# we will balance the training set across steering angles and will only keep -0.5 to +0.5 examples\n",
    "\n",
    "# but this version runs out of memory\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "def balance_array(bin_start,bin_end,bin_size):\n",
    "    '''\n",
    "    This function returns indicies of selected elements \n",
    "    which make the training set balanced\n",
    "    You need to apply the returned index to all training arrays  \n",
    "    '''\n",
    "    num_bins = int((bin_end - bin_start) / bin_size) + 1\n",
    "    min_count = np.min(np.histogram(Y, bins=num_bins, range=(bin_start, bin_end))[0])\n",
    "    balanced_array = []\n",
    "    selected = []\n",
    "\n",
    "    for start in np.arange(bin_start, bin_end, bin_size):\n",
    "        end = start + bin_size\n",
    "        indices = np.where((Y >= start) & (Y < end))[0]\n",
    "        #balanced_array.extend(Y[indices[:min_count]])\n",
    "        selected.extend(indices[:min_count])\n",
    "    return selected\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    # a rough balancing by reducing number of zero steer examples\n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    Y.append(y)\n",
    "\n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "balanced_subset = balance_array(-0.5,0.5,0.05)\n",
    "\n",
    "X = X[balanced_subset]\n",
    "X1 = X1[balanced_subset]\n",
    "Y = Y[balanced_subset]\n",
    "\n",
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how training set is distributed:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 180, 320, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 90, 160, 64)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 56320)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 56321)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           3604608     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,680,321\n",
      "Trainable params: 3,680,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "422/422 [==============================] - 477s 1s/step - loss: 0.0477 - MSE: 0.0477 - val_loss: 0.0166 - val_MSE: 0.0166\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 478s 1s/step - loss: 0.0103 - MSE: 0.0103 - val_loss: 0.0867 - val_MSE: 0.0867\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 480s 1s/step - loss: 0.0629 - MSE: 0.0629 - val_loss: 0.1341 - val_MSE: 0.1341\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 455s 1s/step - loss: 0.0550 - MSE: 0.0550 - val_loss: 0.1439 - val_MSE: 0.1439\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 446s 1s/step - loss: 0.0539 - MSE: 0.0539 - val_loss: 0.1474 - val_MSE: 0.1474\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 441s 1s/step - loss: 0.0536 - MSE: 0.0536 - val_loss: 0.1490 - val_MSE: 0.1490\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 434s 1s/step - loss: 0.0535 - MSE: 0.0535 - val_loss: 0.1500 - val_MSE: 0.1500\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 428s 1s/step - loss: 0.0535 - MSE: 0.0535 - val_loss: 0.1505 - val_MSE: 0.1505\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 442s 1s/step - loss: 0.0535 - MSE: 0.0535 - val_loss: 0.1509 - val_MSE: 0.1509\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 441s 1s/step - loss: 0.0534 - MSE: 0.0534 - val_loss: 0.1511 - val_MSE: 0.1511\n",
      "264/264 [==============================] - 110s 416ms/step\n",
      "Prediction min:  -0.0011058366  Prediction max:  0.008240545\n"
     ]
    }
   ],
   "source": [
    "# attempt 5\n",
    "# we will exclude most of zero steer examples before we add examples to our arrays\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "EVERY_WHAT = 7\n",
    "\n",
    "def balance_array(bin_start,bin_end,bin_size):\n",
    "    '''\n",
    "    This function returns indicies of selected elements \n",
    "    which make the training set balanced\n",
    "    You need to apply the returned index to all training arrays  \n",
    "    '''\n",
    "    num_bins = int((bin_end - bin_start) / bin_size) + 1\n",
    "    min_count = np.min(np.histogram(Y, bins=num_bins, range=(bin_start, bin_end))[0])\n",
    "    balanced_array = []\n",
    "    selected = []\n",
    "\n",
    "    for start in np.arange(bin_start, bin_end, bin_size):\n",
    "        end = start + bin_size\n",
    "        indices = np.where((Y >= start) & (Y < end))[0]\n",
    "        #balanced_array.extend(Y[indices[:min_count]])\n",
    "        selected.extend(indices[:min_count])\n",
    "    return selected\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "straight_counter = 0 # we will be counting images to limit by applying \"every n'th\"\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    if (abs(y)>0.05 or straight_counter % EVERY_WHAT ==0) and abs(y)<0.5:\n",
    "        X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "        # gen direction values are taken from after 1st '_' in file name\n",
    "        X1.append(int(example.split('_')[1]))\n",
    "        Y.append(y)\n",
    "    if abs(y)>=0.05:\n",
    "        straight_counter +=1\n",
    "\n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "balanced_subset = balance_array(-0.5,0.5,0.05)\n",
    "\n",
    "X = X[balanced_subset]\n",
    "X1 = X1[balanced_subset]\n",
    "Y = Y[balanced_subset]\n",
    "\n",
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "106/106 [==============================] - 438s 4s/step - loss: 0.0627 - MSE: 0.0627 - val_loss: 0.1792 - val_MSE: 0.1792\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 437s 4s/step - loss: 0.0560 - MSE: 0.0560 - val_loss: 0.2070 - val_MSE: 0.2070\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 435s 4s/step - loss: 0.0534 - MSE: 0.0534 - val_loss: 0.2260 - val_MSE: 0.2260\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 435s 4s/step - loss: 0.0525 - MSE: 0.0525 - val_loss: 0.2369 - val_MSE: 0.2369\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 446s 4s/step - loss: 0.0522 - MSE: 0.0522 - val_loss: 0.2442 - val_MSE: 0.2442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17158e0a508>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, X1], Y, batch_size=64, shuffle=True, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 180, 320, 64  1792        ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 90, 160, 64)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 56320)        0           ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 56321)        0           ['flatten_1[0][0]',              \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           3604608     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,680,321\n",
      "Trainable params: 3,680,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "211/211 [==============================] - 523s 2s/step - loss: 0.4639 - val_loss: 0.0893\n",
      "Epoch 2/5\n",
      "211/211 [==============================] - 521s 2s/step - loss: 0.0485 - val_loss: 0.0293\n",
      "Epoch 3/5\n",
      "211/211 [==============================] - 530s 3s/step - loss: 0.0286 - val_loss: 0.0367\n",
      "Epoch 4/5\n",
      "211/211 [==============================] - 523s 2s/step - loss: 0.0246 - val_loss: 0.0171\n",
      "Epoch 5/5\n",
      "211/211 [==============================] - 525s 2s/step - loss: 0.0214 - val_loss: 0.0152\n",
      "264/264 [==============================] - 111s 421ms/step\n",
      "Prediction min:  0.27616963  Prediction max:  0.3254479\n"
     ]
    }
   ],
   "source": [
    "# creating a version with regularisers\n",
    "from keras import regularizers\n",
    "# adding batch norm\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu',activity_regularizer=regularizers.L2(1e-5))(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "model.fit([X, X1], Y, batch_size=32, shuffle=False, epochs=5, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how training set is distributed:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlHElEQVR4nO3de3BU5cHH8d+GJJtw2cQEs2tqQrFaIApioSTrpbaYEmlwYMy0SgOmTkY6TEIrodamQwFDa5DBSnGiVEvBjjJUOsW2EVEuCq0sAWKZiRCpFzqJwiYNkYQg2dzO+4cvq8tNl2ySZ5fvZ+bMuOc8++xzjgx8Z3M2a7MsyxIAAIBBogZ6AQAAAGcjUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYJ3qgF3Apenp6dPToUQ0bNkw2m22glwMAAL4Ey7J08uRJpaamKirq4u+RhGWgHD16VGlpaQO9DAAAcAnq6+t19dVXX3RMWAbKsGHDJH16gg6HY4BXAwAAvozW1lalpaX5/x2/mLAMlDM/1nE4HAQKAABh5svcnsFNsgAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDhh+W3G4aqurk5NTU19Mvfw4cOVnp7eJ3MDANDfCJR+UldXp1Gjx6j99Cd9Mn9c/GAdfqeWSAEARAQCpZ80NTWp/fQnSp62QDHJaSGdu/N4vY5XPq6mpiYCBQAQEQiUfhaTnCa769qBXgYAAEbjJlkAAGAcAgUAABgn6ED56KOPNGvWLCUnJys+Pl5jx47V/v37/ccty9KiRYt01VVXKT4+XtnZ2Xr33XcD5mhublZ+fr4cDocSExNVWFiotra23p8NAACICEEFyscff6xbbrlFMTExeuWVV3To0CE9/vjjuuKKK/xjli9frlWrVmn16tWqqqrSkCFDlJOTo/b2dv+Y/Px8HTx4UFu3blVlZaV27dqlOXPmhO6sAABAWAvqJtnHHntMaWlpWrt2rX/fyJEj/f9tWZZWrlyphQsXavr06ZKkP/3pT3I6nXrppZd07733qra2Vlu2bNG+ffs0ceJESdKTTz6p733ve1qxYoVSU1NDcV4AACCMBfUOyt///ndNnDhR3//+95WSkqKbbrpJzz77rP/4kSNH5PV6lZ2d7d+XkJCgzMxMeTweSZLH41FiYqI/TiQpOztbUVFRqqqqOu/r+nw+tba2BmwAACByBRUoH3zwgZ5++mldd911evXVVzV37lz95Cc/0XPPPSdJ8nq9kiSn0xnwPKfT6T/m9XqVkpIScDw6OlpJSUn+MWcrLy9XQkKCf0tLC+3vEQEAAGYJKlB6enr0jW98Q48++qhuuukmzZkzRw888IBWr17dV+uTJJWWlqqlpcW/1dfX9+nrAQCAgRVUoFx11VXKyMgI2DdmzBjV1dVJklwulySpoaEhYExDQ4P/mMvlUmNjY8Dxrq4uNTc3+8eczW63y+FwBGwAACByBRUot9xyiw4fPhyw7z//+Y9GjBgh6dMbZl0ul7Zv3+4/3traqqqqKrndbkmS2+3WiRMnVF1d7R+zY8cO9fT0KDMz85JPBAAARI6gPsUzf/583XzzzXr00Uf1gx/8QHv37tUzzzyjZ555RpJks9n04IMP6te//rWuu+46jRw5Ur/61a+UmpqqGTNmSPr0HZc777zT/6Ohzs5OFRcX69577+UTPAAAQFKQgfLNb35TmzZtUmlpqcrKyjRy5EitXLlS+fn5/jE///nPderUKc2ZM0cnTpzQrbfeqi1btiguLs4/5oUXXlBxcbHuuOMORUVFKS8vT6tWrQrdWQEAgLAW9JcFTps2TdOmTbvgcZvNprKyMpWVlV1wTFJSktavXx/sSwMAgMsE38UDAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAONED/QCAAAwQV1dnZqamkI+7/Dhw5Wenh7yeSMdgQIAuOzV1dVp1Ogxaj/9ScjnjosfrMPv1BIpQSJQAACXvaamJrWf/kTJ0xYoJjktZPN2Hq/X8crH1dTURKAEiUABAOD/xSSnye66dqCXAXGTLAAAMBCBAgAAjEOgAAAA4xAoAADAOAQKAAAwDp/iQcTpq1+2JPELlwCgvxAoiCh9+cuWJH7hEgD0FwIFEaWvftmSxC9cAoD+RKAgIvHLlgAgvHGTLAAAMA6BAgAAjEOgAAAA4wQVKEuWLJHNZgvYRo8e7T/e3t6uoqIiJScna+jQocrLy1NDQ0PAHHV1dcrNzdXgwYOVkpKihx56SF1dXaE5GwAAEBGCvkn2+uuv17Zt2z6bIPqzKebPn6+XX35ZGzduVEJCgoqLi3X33XfrzTfflCR1d3crNzdXLpdLu3fv1rFjx3TfffcpJiZGjz76aAhOBwAARIKgAyU6Oloul+uc/S0tLVqzZo3Wr1+vyZMnS5LWrl2rMWPGaM+ePcrKytJrr72mQ4cOadu2bXI6nRo/fryWLl2qhx9+WEuWLFFsbGzvzwgAAIS9oO9Beffdd5WamqprrrlG+fn5qqurkyRVV1ers7NT2dnZ/rGjR49Wenq6PB6PJMnj8Wjs2LFyOp3+MTk5OWptbdXBgwd7ey4AACBCBPUOSmZmptatW6dRo0bp2LFjeuSRR3Tbbbfp7bffltfrVWxsrBITEwOe43Q65fV6JUlerzcgTs4cP3PsQnw+n3w+n/9xa2trMMsGAABhJqhAmTp1qv+/x40bp8zMTI0YMUIvvvii4uPjQ764M8rLy/XII4/02fwAAMAsvfqYcWJior7+9a/rvffek8vlUkdHh06cOBEwpqGhwX/PisvlOudTPWcen+++ljNKS0vV0tLi3+rr63uzbAAAYLheBUpbW5vef/99XXXVVZowYYJiYmK0fft2//HDhw+rrq5ObrdbkuR2u1VTU6PGxkb/mK1bt8rhcCgjI+OCr2O32+VwOAI2AAAQuYL6Ec/PfvYz3XXXXRoxYoSOHj2qxYsXa9CgQZo5c6YSEhJUWFiokpISJSUlyeFwaN68eXK73crKypIkTZkyRRkZGZo9e7aWL18ur9erhQsXqqioSHa7vU9OEAAAhJ+gAuXDDz/UzJkzdfz4cV155ZW69dZbtWfPHl155ZWSpCeeeEJRUVHKy8uTz+dTTk6OnnrqKf/zBw0apMrKSs2dO1dut1tDhgxRQUGBysrKQntWAAAgrAUVKBs2bLjo8bi4OFVUVKiiouKCY0aMGKHNmzcH87IAAOAyw3fxAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADBO9EAvAKFTW1sb8jmHDx+u9PT0kM8LAMDFECgRoLvtY8lm06xZs0I+d1z8YB1+p5ZIAQD0KwIlAvT42iTLUvK0BYpJTgvZvJ3H63W88nE1NTURKACAfkWgRJCY5DTZXdcO9DIAAOg1bpIFAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYhy8LPI+6ujo1NTWFdM7a2tqQzhcJuM6f6YtrcYbP55Pdbg+ruVlz/8wdjmsePnx4WH67el/93dSX/w8H+loTKGepq6vTqNFj1H76k4FeSkTjOn+mz6+FLUqyesJrbtbcP3OH4Zrj4gfr8Du1YRMp3W0fSzabZs2a1Tcv0If/Dwf6WhMoZ2lqalL76U+UPG2BYpLTQjbv6Q/2q+Wfz4dsvnDHdf5MX10L6bPrEU5zs+b+mTsc19x5vF7HKx9XU1NT2ARKj69Nsqywus6SGdeaQLmAmOQ02V3Xhmy+zuP1IZsrknCdPxPqayF9dj3CaW7W3D9zh+OawxnXOXjcJAsAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwTq8CZdmyZbLZbHrwwQf9+9rb21VUVKTk5GQNHTpUeXl5amhoCHheXV2dcnNzNXjwYKWkpOihhx5SV1dXb5YCAAAiyCUHyr59+/T73/9e48aNC9g/f/58/eMf/9DGjRu1c+dOHT16VHfffbf/eHd3t3Jzc9XR0aHdu3frueee07p167Ro0aJLPwsAABBRLilQ2tralJ+fr2effVZXXHGFf39LS4vWrFmj3/72t5o8ebImTJigtWvXavfu3dqzZ48k6bXXXtOhQ4f0/PPPa/z48Zo6daqWLl2qiooKdXR0hOasAABAWLukQCkqKlJubq6ys7MD9ldXV6uzszNg/+jRo5Weni6PxyNJ8ng8Gjt2rJxOp39MTk6OWltbdfDgwfO+ns/nU2tra8AGAAAiV9BfFrhhwwa99dZb2rdv3znHvF6vYmNjlZiYGLDf6XTK6/X6x3w+Ts4cP3PsfMrLy/XII48Eu1QAABCmgnoHpb6+Xj/96U/1wgsvKC4urq/WdI7S0lK1tLT4t/r68P3GWgAA8MWCCpTq6mo1NjbqG9/4hqKjoxUdHa2dO3dq1apVio6OltPpVEdHh06cOBHwvIaGBrlcLkmSy+U651M9Zx6fGXM2u90uh8MRsAEAgMgVVKDccccdqqmp0YEDB/zbxIkTlZ+f7//vmJgYbd++3f+cw4cPq66uTm63W5LkdrtVU1OjxsZG/5itW7fK4XAoIyMjRKcFAADCWVD3oAwbNkw33HBDwL4hQ4YoOTnZv7+wsFAlJSVKSkqSw+HQvHnz5Ha7lZWVJUmaMmWKMjIyNHv2bC1fvlxer1cLFy5UUVGR7HZ7iE4LAACEs6Bvkv0iTzzxhKKiopSXlyefz6ecnBw99dRT/uODBg1SZWWl5s6dK7fbrSFDhqigoEBlZWWhXgoAAAhTvQ6UN954I+BxXFycKioqVFFRccHnjBgxQps3b+7tSwMAgAjFd/EAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOEEFytNPP61x48bJ4XDI4XDI7XbrlVde8R9vb29XUVGRkpOTNXToUOXl5amhoSFgjrq6OuXm5mrw4MFKSUnRQw89pK6urtCcDQAAiAhBBcrVV1+tZcuWqbq6Wvv379fkyZM1ffp0HTx4UJI0f/58/eMf/9DGjRu1c+dOHT16VHfffbf/+d3d3crNzVVHR4d2796t5557TuvWrdOiRYtCe1YAACCsRQcz+K677gp4/Jvf/EZPP/209uzZo6uvvlpr1qzR+vXrNXnyZEnS2rVrNWbMGO3Zs0dZWVl67bXXdOjQIW3btk1Op1Pjx4/X0qVL9fDDD2vJkiWKjY0N3ZkBfaS2ttbo+QAgEgQVKJ/X3d2tjRs36tSpU3K73aqurlZnZ6eys7P9Y0aPHq309HR5PB5lZWXJ4/Fo7Nixcjqd/jE5OTmaO3euDh48qJtuuum8r+Xz+eTz+fyPW1tbL3XZwCXrbvtYstk0a9asgV4KAES8oAOlpqZGbrdb7e3tGjp0qDZt2qSMjAwdOHBAsbGxSkxMDBjvdDrl9XolSV6vNyBOzhw/c+xCysvL9cgjjwS7VCCkenxtkmUpedoCxSSnhWze0x/sV8s/nw/ZfAAQCYIOlFGjRunAgQNqaWnRX/7yFxUUFGjnzp19sTa/0tJSlZSU+B+3trYqLS10/0AAwYhJTpPddW3I5us8Xh+yuQAgUgQdKLGxsbr22k//cp4wYYL27dun3/3ud7rnnnvU0dGhEydOBLyL0tDQIJfLJUlyuVzau3dvwHxnPuVzZsz52O122e32YJcKAADCVK9/D0pPT498Pp8mTJigmJgYbd++3X/s8OHDqqurk9vtliS53W7V1NSosbHRP2br1q1yOBzKyMjo7VIAAECECOodlNLSUk2dOlXp6ek6efKk1q9frzfeeEOvvvqqEhISVFhYqJKSEiUlJcnhcGjevHlyu93KysqSJE2ZMkUZGRmaPXu2li9fLq/Xq4ULF6qoqIh3SAAAgF9QgdLY2Kj77rtPx44dU0JCgsaNG6dXX31V3/3udyVJTzzxhKKiopSXlyefz6ecnBw99dRT/ucPGjRIlZWVmjt3rtxut4YMGaKCggKVlZWF9qwAAEBYCypQ1qxZc9HjcXFxqqioUEVFxQXHjBgxQps3bw7mZQEAwGWG7+IBAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHGiB3oBAAAEo7a2NizmRO8QKACAsNDd9rFks2nWrFkDvRT0AwIFABAWenxtkmUpedoCxSSnhXTu0x/sV8s/nw/pnOgdAgUAEFZiktNkd10b0jk7j9eHdD70HjfJAgAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDjRA70AmK+2tjYs5gQARA4CBRfU3faxZLNp1qxZA70UAMBlJqhAKS8v11//+le98847io+P180336zHHntMo0aN8o9pb2/XggULtGHDBvl8PuXk5Oipp56S0+n0j6mrq9PcuXP1+uuva+jQoSooKFB5ebmio+klk/T42iTLUvK0BYpJTgvp3Kc/2K+Wfz4f0jkBAJEjqCLYuXOnioqK9M1vflNdXV365S9/qSlTpujQoUMaMmSIJGn+/Pl6+eWXtXHjRiUkJKi4uFh333233nzzTUlSd3e3cnNz5XK5tHv3bh07dkz33XefYmJi9Oijj4b+DNFrMclpsruuDemcncfrQzofACCyBBUoW7ZsCXi8bt06paSkqLq6Wt/61rfU0tKiNWvWaP369Zo8ebIkae3atRozZoz27NmjrKwsvfbaazp06JC2bdsmp9Op8ePHa+nSpXr44Ye1ZMkSxcbGhu7sAABAWOrVp3haWlokSUlJSZKk6upqdXZ2Kjs72z9m9OjRSk9Pl8fjkSR5PB6NHTs24Ec+OTk5am1t1cGDB8/7Oj6fT62trQEbAACIXJccKD09PXrwwQd1yy236IYbbpAkeb1excbGKjExMWCs0+mU1+v1j/l8nJw5fubY+ZSXlyshIcG/paWF9n4IAABglksOlKKiIr399tvasGFDKNdzXqWlpWppafFv9fXcvwAAQCS7pI/NFBcXq7KyUrt27dLVV1/t3+9yudTR0aETJ04EvIvS0NAgl8vlH7N3796A+RoaGvzHzsdut8tut1/KUgEAQBgK6h0Uy7JUXFysTZs2aceOHRo5cmTA8QkTJigmJkbbt2/37zt8+LDq6urkdrslSW63WzU1NWpsbPSP2bp1qxwOhzIyMnpzLgAAIEIE9Q5KUVGR1q9fr7/97W8aNmyY/56RhIQExcfHKyEhQYWFhSopKVFSUpIcDofmzZsnt9utrKwsSdKUKVOUkZGh2bNna/ny5fJ6vVq4cKGKiop4lwQAAEgKMlCefvppSdK3v/3tgP1r167Vj370I0nSE088oaioKOXl5QX8orYzBg0apMrKSs2dO1dut1tDhgxRQUGBysrKencmAAAgYgQVKJZlfeGYuLg4VVRUqKKi4oJjRowYoc2bNwfz0gAA4DLCtxkDAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOEEHyq5du3TXXXcpNTVVNptNL730UsBxy7K0aNEiXXXVVYqPj1d2drbefffdgDHNzc3Kz8+Xw+FQYmKiCgsL1dbW1qsTAQAAkSPoQDl16pRuvPFGVVRUnPf48uXLtWrVKq1evVpVVVUaMmSIcnJy1N7e7h+Tn5+vgwcPauvWraqsrNSuXbs0Z86cSz8LAAAQUaKDfcLUqVM1derU8x6zLEsrV67UwoULNX36dEnSn/70JzmdTr300ku69957VVtbqy1btmjfvn2aOHGiJOnJJ5/U9773Pa1YsUKpqam9OB0AABAJQnoPypEjR+T1epWdne3fl5CQoMzMTHk8HkmSx+NRYmKiP04kKTs7W1FRUaqqqjrvvD6fT62trQEbAACIXCENFK/XK0lyOp0B+51Op/+Y1+tVSkpKwPHo6GglJSX5x5ytvLxcCQkJ/i0tLS2UywYAAIYJi0/xlJaWqqWlxb/V19cP9JIAAEAfCmmguFwuSVJDQ0PA/oaGBv8xl8ulxsbGgONdXV1qbm72jzmb3W6Xw+EI2AAAQOQKaaCMHDlSLpdL27dv9+9rbW1VVVWV3G63JMntduvEiROqrq72j9mxY4d6enqUmZkZyuUAAIAwFfSneNra2vTee+/5Hx85ckQHDhxQUlKS0tPT9eCDD+rXv/61rrvuOo0cOVK/+tWvlJqaqhkzZkiSxowZozvvvFMPPPCAVq9erc7OThUXF+vee+/lEzwAAEDSJQTK/v379Z3vfMf/uKSkRJJUUFCgdevW6ec//7lOnTqlOXPm6MSJE7r11lu1ZcsWxcXF+Z/zwgsvqLi4WHfccYeioqKUl5enVatWheB0AABAJAg6UL797W/LsqwLHrfZbCorK1NZWdkFxyQlJWn9+vXBvjQAALhMhMWneAAAwOWFQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGGdBAqaio0Fe/+lXFxcUpMzNTe/fuHcjlAAAAQwxYoPz5z39WSUmJFi9erLfeeks33nijcnJy1NjYOFBLAgAAhhiwQPntb3+rBx54QPfff78yMjK0evVqDR48WH/84x8HakkAAMAQ0QPxoh0dHaqurlZpaal/X1RUlLKzs+XxeM4Z7/P55PP5/I9bWlokSa2trSFfW1tb26ev6X1PPR3tIZu383h9n8zbl3Oz5v6ZOxzX3Jdzs+b+mZs198/c4bhmSeps/lDSp/8mhvLf2jNzWZb1xYOtAfDRRx9Zkqzdu3cH7H/ooYesSZMmnTN+8eLFliQ2NjY2Nja2CNjq6+u/sBUG5B2UYJWWlqqkpMT/uKenR83NzUpOTpbNZhvAlfWv1tZWpaWlqb6+Xg6HY6CXc1ngmvc/rnn/45r3v8v1mluWpZMnTyo1NfULxw5IoAwfPlyDBg1SQ0NDwP6Ghga5XK5zxtvtdtnt9oB9iYmJfblEozkcjsvqD7QJuOb9j2ve/7jm/e9yvOYJCQlfatyA3CQbGxurCRMmaPv27f59PT092r59u9xu90AsCQAAGGTAfsRTUlKigoICTZw4UZMmTdLKlSt16tQp3X///QO1JAAAYIgBC5R77rlH//vf/7Ro0SJ5vV6NHz9eW7ZskdPpHKglGc9ut2vx4sXn/LgLfYdr3v+45v2Pa97/uOZfzGZZX+azPgAAAP2H7+IBAADGIVAAAIBxCBQAAGAcAgUAABiHQDFcc3Oz8vPz5XA4lJiYqMLCQv/3BX0Ry7I0depU2Ww2vfTSS3270AgS7DVvbm7WvHnzNGrUKMXHxys9PV0/+clP/N8ZhXNVVFToq1/9quLi4pSZmam9e/dedPzGjRs1evRoxcXFaezYsdq8eXM/rTRyBHPNn332Wd1222264oordMUVVyg7O/sL/x/hXMH+OT9jw4YNstlsmjFjRt8u0HAEiuHy8/N18OBBbd26VZWVldq1a5fmzJnzpZ67cuXKy+qrAEIl2Gt+9OhRHT16VCtWrNDbb7+tdevWacuWLSosLOzHVYePP//5zyopKdHixYv11ltv6cYbb1ROTo4aGxvPO3737t2aOXOmCgsL9e9//1szZszQjBkz9Pbbb/fzysNXsNf8jTfe0MyZM/X666/L4/EoLS1NU6ZM0UcffdTPKw9fwV7zM/773//qZz/7mW677bZ+WqnBQvLtf+gThw4dsiRZ+/bt8+975ZVXLJvNZn300UcXfe6///1v6ytf+Yp17NgxS5K1adOmPl5tZOjNNf+8F1980YqNjbU6Ozv7YplhbdKkSVZRUZH/cXd3t5WammqVl5efd/wPfvADKzc3N2BfZmam9eMf/7hP1xlJgr3mZ+vq6rKGDRtmPffcc321xIhzKde8q6vLuvnmm60//OEPVkFBgTV9+vR+WKm5eAfFYB6PR4mJiZo4caJ/X3Z2tqKiolRVVXXB533yySf64Q9/qIqKivN+txEu7FKv+dlaWlrkcDgUHR0W38fZbzo6OlRdXa3s7Gz/vqioKGVnZ8vj8Zz3OR6PJ2C8JOXk5FxwPAJdyjU/2yeffKLOzk4lJSX11TIjyqVe87KyMqWkpPDu6//jb0+Deb1epaSkBOyLjo5WUlKSvF7vBZ83f/583XzzzZo+fXpfLzHiXOo1/7ympiYtXbr0S/8o7nLS1NSk7u7uc35jtNPp1DvvvHPe53i93vOO/7L/Py53l3LNz/bwww8rNTX1nFDE+V3KNf/Xv/6lNWvW6MCBA/2wwvDAOygD4Be/+IVsNttFty/7F8fZ/v73v2vHjh1auXJlaBcd5vrymn9ea2urcnNzlZGRoSVLlvR+4cAAW7ZsmTZs2KBNmzYpLi5uoJcTkU6ePKnZs2fr2Wef1fDhwwd6OcbgHZQBsGDBAv3oRz+66JhrrrlGLpfrnBuqurq61NzcfMEf3ezYsUPvv/++EhMTA/bn5eXptttu0xtvvNGLlYevvrzmZ5w8eVJ33nmnhg0bpk2bNikmJqa3y444w4cP16BBg9TQ0BCwv6Gh4YLX1+VyBTUegS7lmp+xYsUKLVu2TNu2bdO4ceP6cpkRJdhr/v777+u///2v7rrrLv++np4eSZ++g3v48GF97Wtf69tFm2igb4LBhZ25YXP//v3+fa+++upFb9g8duyYVVNTE7BJsn73u99ZH3zwQX8tPWxdyjW3LMtqaWmxsrKyrNtvv906depUfyw1bE2aNMkqLi72P+7u7ra+8pWvXPQm2WnTpgXsc7vd3CQbhGCvuWVZ1mOPPWY5HA7L4/H0xxIjTjDX/PTp0+f8vT19+nRr8uTJVk1NjeXz+fpz6cYgUAx35513WjfddJNVVVVl/etf/7Kuu+46a+bMmf7jH374oTVq1CirqqrqgnOIT/EEJdhr3tLSYmVmZlpjx4613nvvPevYsWP+raura6BOw1gbNmyw7Ha7tW7dOuvQoUPWnDlzrMTERMvr9VqWZVmzZ8+2fvGLX/jHv/nmm1Z0dLS1YsUKq7a21lq8eLEVExNj1dTUDNQphJ1gr/myZcus2NhY6y9/+UvAn+eTJ08O1CmEnWCv+dn4FA+BYrzjx49bM2fOtIYOHWo5HA7r/vvvD/hL4siRI5Yk6/XXX7/gHARKcIK95q+//rol6bzbkSNHBuYkDPfkk09a6enpVmxsrDVp0iRrz549/mO33367VVBQEDD+xRdftL7+9a9bsbGx1vXXX2+9/PLL/bzi8BfMNR8xYsR5/zwvXry4/xcexoL9c/55BIpl2SzLsvr7x0oAAAAXw6d4AACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxvk/oFpQ5eRi1qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 180, 320, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 90, 160, 64)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 22, 40, 128)  8320        ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 22, 40, 32)   4128        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 22, 40, 4)    132         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3520)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3521)         0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            3522        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 91,750\n",
      "Trainable params: 91,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "211/211 [==============================] - 517s 2s/step - loss: 0.0161 - val_loss: 0.0713\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 503s 2s/step - loss: 0.0411 - val_loss: 0.0155\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 507s 2s/step - loss: 0.0344 - val_loss: 0.0914\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 508s 2s/step - loss: 0.0303 - val_loss: 0.0818\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 497s 2s/step - loss: 0.0297 - val_loss: 0.0770\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 493s 2s/step - loss: 0.0296 - val_loss: 0.0744\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 508s 2s/step - loss: 0.0295 - val_loss: 0.0731\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 497s 2s/step - loss: 0.0296 - val_loss: 0.0724\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 493s 2s/step - loss: 0.0296 - val_loss: 0.0720\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 493s 2s/step - loss: 0.0296 - val_loss: 0.0717\n",
      "264/264 [==============================] - 108s 408ms/step\n",
      "Prediction min:  -0.22290976  Prediction max:  0.22751616\n"
     ]
    }
   ],
   "source": [
    "# attempt 6\n",
    "# we will exclude most of zero steer examples before we add examples to our arrays\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "EVERY_WHAT = 7\n",
    "\n",
    "def balance_array(bin_start,bin_end,bin_size):\n",
    "    '''\n",
    "    This function returns indicies of selected elements \n",
    "    which make the training set balanced\n",
    "    You need to apply the returned index to all training arrays  \n",
    "    '''\n",
    "    num_bins = int((bin_end - bin_start) / bin_size) + 1\n",
    "    min_count = np.min(np.histogram(Y, bins=num_bins, range=(bin_start, bin_end))[0])\n",
    "    balanced_array = []\n",
    "    selected = []\n",
    "\n",
    "    for start in np.arange(bin_start, bin_end, bin_size):\n",
    "        end = start + bin_size\n",
    "        indices = np.where((Y >= start) & (Y < end))[0]\n",
    "        #balanced_array.extend(Y[indices[:min_count]])\n",
    "        selected.extend(indices[:min_count])\n",
    "    return selected\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dense(32, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dense(4, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    output = Dense(1, activation='linear')(concatenated_inputs)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "straight_counter = 0 # we will be counting images to limit by applying \"every n'th\"\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    if (abs(y)>0.05 or straight_counter % EVERY_WHAT ==0) and abs(y)<0.5:\n",
    "        X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "        # gen direction values are taken from after 1st '_' in file name\n",
    "        X1.append(int(example.split('_')[1]))\n",
    "        Y.append(y)\n",
    "    if abs(y)>=0.05:\n",
    "        straight_counter +=1\n",
    "\n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "balanced_subset = balance_array(-0.5,0.5,0.05)\n",
    "\n",
    "X = X[balanced_subset]\n",
    "X1 = X1[balanced_subset]\n",
    "Y = Y[balanced_subset]\n",
    "\n",
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=32, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())\n",
    "# draw how Predictions are distributed \n",
    "frq, edges = np.histogram(predictions, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how predictios are distributed:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 180, 320, 64  1792        ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 90, 160, 64)  0          ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 22, 40, 128)  8320        ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 22, 40, 128)  0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 22, 40, 4)    516         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 3520)         0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3521)         0           ['flatten_2[0][0]',              \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            3522        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 88,006\n",
      "Trainable params: 88,006\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "211/211 [==============================] - 450s 2s/step - loss: 0.0050 - val_loss: 0.0172\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 436s 2s/step - loss: 0.0183 - val_loss: 0.0917\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 426s 2s/step - loss: 0.0389 - val_loss: 0.0174\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 425s 2s/step - loss: 0.0194 - val_loss: 0.0947\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 424s 2s/step - loss: 0.0294 - val_loss: 0.0788\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 424s 2s/step - loss: 0.0326 - val_loss: 0.0774\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 424s 2s/step - loss: 0.0300 - val_loss: 0.0748\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 425s 2s/step - loss: 0.0296 - val_loss: 0.0732\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 424s 2s/step - loss: 0.0296 - val_loss: 0.0724\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 437s 2s/step - loss: 0.0296 - val_loss: 0.0719\n",
      "264/264 [==============================] - 113s 426ms/step\n",
      "Prediction min:  -0.22394256  Prediction max:  0.22738229\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dropout(0.1) (x)\n",
    "    x = Dense(4, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    output = Dense(1, activation='linear')(concatenated_inputs)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=32, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())\n",
    "# draw how Predictions are distributed \n",
    "frq, edges = np.histogram(predictions, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how predictios are distributed:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how predictios are distributed:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw how Predictions are distributed \n",
    "frq, edges = np.histogram(predictions, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how predictios are distributed:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15212\\1912276769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmypath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;31m# option to make images smaller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWIDTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# attempt 7\n",
    "# this is after changing image generation approach\n",
    "# where we added spins at various angles at each point\n",
    "# we do not need to exclude some straight images any more\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "MAX_STEER_DEGREES = 40 # e.g. steering goes from -40 to +40\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dense(32, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dense(4, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    output = Dense(1, activation='linear')(concatenated_inputs)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y > MAX_STEER_DEGREES:\n",
    "        y = MAX_STEER_DEGREES\n",
    "    elif y < -MAX_STEER_DEGREES:\n",
    "        y = -MAX_STEER_DEGREES\n",
    "    \n",
    "    y = float(y)/MAX_STEER_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    \n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    Y.append(y)\n",
    "    \n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=32, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())\n",
    "# draw how Predictions are distributed \n",
    "frq, edges = np.histogram(predictions, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how predictios are distributed:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_Spin\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_Spin\\assets\n"
     ]
    }
   ],
   "source": [
    "# to save model\n",
    "model.save(\"GPS_Visual_Model_Spin\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 75.0 GiB for an array with shape (58241, 180, 320, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14356\\1927560334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m#convert to numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[0mX1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 75.0 GiB for an array with shape (58241, 180, 320, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "# attempt 8\n",
    "# this is after changing image generation approach\n",
    "# where we added spins at various angles at each point\n",
    "# and revised \"general/GPS direction\" where only options to get\n",
    "# left or right are:\n",
    "# 1. intersection ahead and we are turning\n",
    "# 2. lane changes - just before\n",
    "# we do not need to exclude some straight images any more\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "\n",
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "MAX_STEER_DEGREES = 40 # e.g. steering goes from -40 to +40\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dense(32, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dense(4, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    output = Dense(1, activation='linear')(concatenated_inputs)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y > MAX_STEER_DEGREES:\n",
    "        y = MAX_STEER_DEGREES\n",
    "    elif y < -MAX_STEER_DEGREES:\n",
    "        y = -MAX_STEER_DEGREES\n",
    "    \n",
    "    y = float(y)/MAX_STEER_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    \n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    Y.append(y)\n",
    "    \n",
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=32, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())\n",
    "# draw how Predictions are distributed \n",
    "frq, edges = np.histogram(predictions, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how predictios are distributed:')\n",
    "plt.show()\n",
    "# to save model\n",
    "model.save(\"GPS_Visual_Model_SpinV2\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 180, 320, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 90, 160, 64)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 22, 40, 128)  8320        ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 22, 40, 128)  0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 22, 40, 4)    516         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3520)         0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3521)         0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            3522        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 88,006\n",
      "Trainable params: 88,006\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "2975/2975 [==============================] - 499s 165ms/step - loss: 0.0272 - val_loss: 0.0227\n",
      "Epoch 2/15\n",
      "2975/2975 [==============================] - 463s 156ms/step - loss: 0.0184 - val_loss: 0.0172\n",
      "Epoch 3/15\n",
      "2975/2975 [==============================] - 457s 154ms/step - loss: 0.0165 - val_loss: 0.0168\n",
      "Epoch 4/15\n",
      "2975/2975 [==============================] - 455s 153ms/step - loss: 0.0147 - val_loss: 0.0152\n",
      "Epoch 5/15\n",
      "2975/2975 [==============================] - 461s 155ms/step - loss: 0.0135 - val_loss: 0.0150\n",
      "Epoch 6/15\n",
      "2975/2975 [==============================] - 458s 154ms/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 7/15\n",
      "2975/2975 [==============================] - 448s 151ms/step - loss: 0.0121 - val_loss: 0.0142\n",
      "Epoch 8/15\n",
      "2975/2975 [==============================] - 442s 149ms/step - loss: 0.0118 - val_loss: 0.0143\n",
      "Epoch 9/15\n",
      "2975/2975 [==============================] - 443s 149ms/step - loss: 0.0113 - val_loss: 0.0128\n",
      "Epoch 10/15\n",
      "2975/2975 [==============================] - 442s 149ms/step - loss: 0.0114 - val_loss: 0.0138\n",
      "Epoch 11/15\n",
      "2975/2975 [==============================] - 442s 149ms/step - loss: 0.0112 - val_loss: 0.0133\n",
      "Epoch 12/15\n",
      "2975/2975 [==============================] - 454s 152ms/step - loss: 0.0105 - val_loss: 0.0130\n",
      "Epoch 13/15\n",
      "2975/2975 [==============================] - 451s 152ms/step - loss: 0.0106 - val_loss: 0.0130\n",
      "Epoch 14/15\n",
      "2975/2975 [==============================] - 447s 150ms/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 15/15\n",
      "2975/2975 [==============================] - 456s 153ms/step - loss: 0.0097 - val_loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_SpinV2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_SpinV2\\assets\n"
     ]
    }
   ],
   "source": [
    "# building custom generator to process image in batches when training\n",
    "# this avoids loading all images into RAM at once and stalling the process\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "MAX_STEER_DEGREES = 40\n",
    "# Define the path to your image data directory\n",
    "data_dir = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "\n",
    "# Define the parameters for image preprocessing and augmentation\n",
    "batch_size = 32\n",
    "image_size = (WIDTH,HEIGHT)\n",
    "\n",
    "\n",
    "# Define the position of the label in the image file name\n",
    "label_position = -5  # Assuming the label is the fifth character from the end\n",
    "\n",
    "# Create a custom data generator\n",
    "def custom_data_generator(image_files, batch_size):\n",
    "    num_samples = len(image_files)\n",
    "    while True:\n",
    "        indices = np.random.randint(0, num_samples, batch_size)\n",
    "        batch_images = []\n",
    "        batch_input_2 = []\n",
    "        batch_labels = []\n",
    "        for idx in indices:\n",
    "            image_path = image_files[idx]\n",
    "            label = float(os.path.basename(image_path).split('.png')[0].split('_')[2])\n",
    "            if label > MAX_STEER_DEGREES:\n",
    "                label = MAX_STEER_DEGREES\n",
    "            elif label < -MAX_STEER_DEGREES:\n",
    "                label = -MAX_STEER_DEGREES\n",
    "            label = float(label)/MAX_STEER_DEGREES\n",
    "            input_2 = int(os.path.basename(image_path).split('.png')[0].split('_')[1])\n",
    "            image = preprocess_image(image_path)\n",
    "            batch_images.append(image)\n",
    "            batch_input_2.append(input_2)\n",
    "            batch_labels.append(label)\n",
    "        yield [np.array(batch_images), np.array(batch_input_2)], np.array(batch_labels)\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "    image = keras.preprocessing.image.img_to_array(image)\n",
    "    image = image / 255.0  # Normalize pixel values between 0 and 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(4, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    output = Dense(1, activation='linear')(concatenated_inputs)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of image file paths and labels\n",
    "image_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.png')]\n",
    "\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_index = int(len(image_files) * 0.8)  # 80% for training, 20% for validation\n",
    "train_files, val_files = image_files[:split_index], image_files[split_index:]\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = custom_data_generator(train_files, batch_size)\n",
    "val_generator = custom_data_generator(val_files, batch_size)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, steps_per_epoch=len(train_files) // batch_size, epochs=15,\n",
    "          validation_data=val_generator, validation_steps=len(val_files) // batch_size)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"GPS_Visual_Model_SpinV2\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 180, 320, 64  1792        ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 180, 320, 64  256        ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 90, 160, 64)  0          ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 90, 160, 64)  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 45, 80, 64)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 45, 80, 64)  256         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 22, 40, 64)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 22, 40, 128)  8320        ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 22, 40, 128)  0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 22, 40, 4)    516         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 3520)         0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3521)         0           ['flatten_2[0][0]',              \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            3522        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 88,774\n",
      "Trainable params: 88,390\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      " 906/2975 [========>.....................] - ETA: 5:03 - loss: 0.0748"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18308\\2015306519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m model.fit(train_generator, steps_per_epoch=len(train_files) // batch_size, epochs=15,\n\u001b[1;32m--> 115\u001b[1;33m           validation_data=val_generator, validation_steps=len(val_files) // batch_size)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;31m# Save the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \"\"\"\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             raise ValueError(\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m         \u001b[1;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1155\u001b[0m     \"\"\"\n\u001b[0;32m   1156\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vadim\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# adding batch normalisation\n",
    "\n",
    "# It was performing worse than one above- I stopped it\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "MAX_STEER_DEGREES = 40\n",
    "# Define the path to your image data directory\n",
    "data_dir = 'C:/SelfDrive/GPS with Vision/_img'\n",
    "\n",
    "# Define the parameters for image preprocessing and augmentation\n",
    "batch_size = 32\n",
    "image_size = (WIDTH,HEIGHT)\n",
    "\n",
    "\n",
    "# Define the position of the label in the image file name\n",
    "label_position = -5  # Assuming the label is the fifth character from the end\n",
    "\n",
    "# Create a custom data generator\n",
    "def custom_data_generator(image_files, batch_size):\n",
    "    num_samples = len(image_files)\n",
    "    while True:\n",
    "        indices = np.random.randint(0, num_samples, batch_size)\n",
    "        batch_images = []\n",
    "        batch_input_2 = []\n",
    "        batch_labels = []\n",
    "        for idx in indices:\n",
    "            image_path = image_files[idx]\n",
    "            label = float(os.path.basename(image_path).split('.png')[0].split('_')[2])\n",
    "            if label > MAX_STEER_DEGREES:\n",
    "                label = MAX_STEER_DEGREES\n",
    "            elif label < -MAX_STEER_DEGREES:\n",
    "                label = -MAX_STEER_DEGREES\n",
    "            label = float(label)/MAX_STEER_DEGREES\n",
    "            input_2 = int(os.path.basename(image_path).split('.png')[0].split('_')[1])\n",
    "            image = preprocess_image(image_path)\n",
    "            batch_images.append(image)\n",
    "            batch_input_2.append(input_2)\n",
    "            batch_labels.append(label)\n",
    "        yield [np.array(batch_images), np.array(batch_input_2)], np.array(batch_labels)\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "    image = keras.preprocessing.image.img_to_array(image)\n",
    "    image = image / 255.0  # Normalize pixel values between 0 and 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    # Preprocess the image input\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation='relu',activity_regularizer=regularizers.L2(1e-5))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(4, activation='relu',activity_regularizer=regularizers.L2(1e-5),use_bias=True)(x)\n",
    "    x = Flatten()(x)\n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([x, integer_input])\n",
    "    # Dense layers for prediction\n",
    "    output = Dense(1, activation='linear')(concatenated_inputs)\n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of image file paths and labels\n",
    "image_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.png')]\n",
    "\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_index = int(len(image_files) * 0.8)  # 80% for training, 20% for validation\n",
    "train_files, val_files = image_files[:split_index], image_files[split_index:]\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = custom_data_generator(train_files, batch_size)\n",
    "val_generator = custom_data_generator(val_files, batch_size)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, steps_per_epoch=len(train_files) // batch_size, epochs=15,\n",
    "          validation_data=val_generator, validation_steps=len(val_files) // batch_size)\n",
    "\n",
    "# Save the trained model\n",
    "#model.save(\"GPS_Visual_Model_SpinV2\", overwrite=True,include_optimizer=True,\n",
    "#    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 320, 180, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
